{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/anaconda3/envs/neuropoly/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "from utils.training import train\n",
    "from utils.evaluation import score\n",
    "from utils.data import Data, SpikeDetectionDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "expe_seed = 0\n",
    "method = 'Fukumori'\n",
    "parameters = [0]\n",
    "sfreq = 100  # Sampling frequency\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-11 15:40:48.095 | INFO     | utils.data:get_all_datasets:193 - Recover data for Neuropoly_MEEG_database\n",
      "2022-05-11 15:41:49.298 | INFO     | utils.data:get_dataset:167 - Label creation: No Spike / Spikes mapped on labels [0]\n"
     ]
    }
   ],
   "source": [
    "path_root = 'database/'\n",
    "dataset = Data(path_root, 'spikeandwave', 'EEG', True)\n",
    "all_dataset = dataset.all_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "patience = 10\n",
    "\n",
    "assert method in (\"Fukumori\")\n",
    "\n",
    "num_workers = 0  # Number of processes to use for the data loading process; 0 is the main Python process\n",
    "\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_dataset[0]['Neuropoly_MEEG_database']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = all_dataset[1]['Neuropoly_MEEG_database']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([labels for _ in range(50)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reshape(data.shape[0]*data.shape[1], data.shape[2])\n",
    "data = data[:, :, np.newaxis]\n",
    "labels = labels.reshape(labels.shape[0]*labels.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data, labels )\n",
    "data_train, data_val, labels_train, labels_val = train_test_split(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32750, 201, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = SpikeDetectionDataset(data_train, labels_train)\n",
    "dataset_val = SpikeDetectionDataset(data_val, labels_val)\n",
    "dataset_test = SpikeDetectionDataset(data_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32750, 201, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loaders_train = DataLoader(dataset_train, batch_size=batch_size)\n",
    "loaders_val = DataLoader(dataset_val, batch_size=batch_size)\n",
    "loaders_test = DataLoader(dataset_test, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = loaders_train.dataset[0][0].shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.model as model\n",
    "import utils.training as training\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "importlib.reload(training)\n",
    "\n",
    "if method == \"Fukumori\":\n",
    "    model = model.fukumori2021RNN(input_size=input_size)\n",
    "\n",
    "lr = 1e-3  # Learning rate\n",
    "optimizer = Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch \t train_loss \t valid_loss \t train_perf \t valid_perf\n",
      "--------------------------------------------------------------------------------\n",
      "1 \t 0.0000 \t 0.0000 \t0.0000 \t 0.0000\n",
      "best val loss inf -> 0.0000\n",
      "2 \t 0.0000 \t 0.0000 \t0.0000 \t 0.0000\n",
      "3 \t 0.0000 \t 0.0000 \t0.0000 \t 0.0000\n",
      "4 \t 0.0000 \t 0.0000 \t0.0000 \t 0.0000\n",
      "5 \t 0.0000 \t 0.0000 \t0.0000 \t 0.0000\n",
      "6 \t 0.0000 \t 0.0000 \t0.0000 \t 0.0000\n",
      "7 \t 0.0000 \t 0.0000 \t0.0000 \t 0.0000\n",
      "8 \t 0.0000 \t 0.0000 \t0.0000 \t 0.0000\n",
      "9 \t 0.0000 \t 0.0000 \t0.0000 \t 0.0000\n",
      "10 \t 0.0000 \t 0.0000 \t0.0000 \t 0.0000\n",
      "11 \t 0.0000 \t 0.0000 \t0.0000 \t 0.0000\n",
      "Stop training at epoch 11\n",
      "Best val loss : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "best_model, history = training.train(\n",
    "    model,\n",
    "    method,\n",
    "    loaders_train,\n",
    "    loaders_val,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    parameters,\n",
    "    n_epochs,\n",
    "    patience,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A =np.array([0 ,1])\n",
    "np.where(A==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d5da9f740f62f6cca2855afe8a9c4414994d1fb427f851597be295550a50890"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('neuropoly': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
